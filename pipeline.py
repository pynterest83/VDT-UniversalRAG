import uuid
from typing import TypedDict, List, Optional
import os
from dotenv import load_dotenv
from langchain_huggingface import HuggingFaceEmbeddings

load_dotenv()

from langgraph.graph import StateGraph, END, START
from vectorstore import VectorStore

os.environ["LANGCHAIN_TRACING_V2"] = "false"

class GraphState(TypedDict):
    """
    Äá»‹nh nghÄ©a cáº¥u trÃºc dá»¯ liá»‡u cho toÃ n bá»™ pipeline.

    Attributes:
        question: CÃ¢u há»i gá»‘c tá»« ngÆ°á»i dÃ¹ng.
        text_chunks: Danh sÃ¡ch cÃ¡c Ä‘oáº¡n vÄƒn báº£n truy xuáº¥t Ä‘Æ°á»£c.
        answer: CÃ¢u tráº£ lá»i Ä‘Æ°á»£c táº¡o tá»« context.
        caption_query: Query Ä‘á»ƒ tÃ¬m kiáº¿m caption/áº£nh.
        image_info: ThÃ´ng tin áº£nh tÃ¬m Ä‘Æ°á»£c.
        final_answer: CÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng cho ngÆ°á»i dÃ¹ng (text + áº£nh).
        error: ThÃ´ng bÃ¡o lá»—i náº¿u cÃ³.
    """
    question: str
    text_chunks: Optional[List[dict]]
    answer: Optional[str]
    caption_query: Optional[str]
    image_info: Optional[dict]
    final_answer: Optional[str]
    error: Optional[str]


# --- Khá»Ÿi táº¡o cÃ¡c thÃ nh pháº§n ---
def initialize_components():
    """Khá»Ÿi táº¡o embedding model vÃ  vector stores"""
    embedding_model = HuggingFaceEmbeddings(
        model_name="bge-m3-v3"
    )
    
    # Vector store cho context (vÄƒn báº£n)
    context_store = VectorStore(collection_name="universal-rag-precomputed-clean-2")
    
    # Vector store cho caption/áº£nh (giáº£ sá»­ cÃ³ collection khÃ¡c cho áº£nh)
    image_store = VectorStore(collection_name="universal-rag-precomputed-clean-2")
    
    return embedding_model, context_store, image_store

# Khá»Ÿi táº¡o global components
embedding_model, context_store, image_store = initialize_components()

# --- Mock LLM functions ---
def mock_llm_generate_answer(question: str, context_chunks: List[dict]) -> str:
    """Giáº£ láº­p LLM sinh cÃ¢u tráº£ lá»i tá»« context."""
    print("ğŸ¤– >> Giáº£ láº­p LLM: Äang sinh cÃ¢u tráº£ lá»i tá»« context...")
    if not context_chunks:
        return "Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i nÃ y."
    
    context_text = "\n".join([chunk['content'] for chunk in context_chunks])
    answer = f"Dá»±a trÃªn thÃ´ng tin tÃ¬m Ä‘Æ°á»£c, Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i '{question}':\n\n{context_text}"
    return answer

def mock_llm_generate_caption_query(question: str, answer: str) -> str:
    """Giáº£ láº­p LLM táº¡o query Ä‘á»ƒ tÃ¬m kiáº¿m caption/áº£nh."""
    print("ğŸ¤– >> Giáº£ láº­p LLM: Äang táº¡o query tÃ¬m kiáº¿m áº£nh...")
    # Trong thá»±c táº¿, LLM sáº½ phÃ¢n tÃ­ch cÃ¢u há»i vÃ  cÃ¢u tráº£ lá»i Ä‘á»ƒ táº¡o query mÃ´ táº£ áº£nh cáº§n tÃ¬m
    return f"áº£nh minh há»a {question}"

def mock_llm_generate_final_answer(answer: str, image_info: Optional[dict]) -> str:
    """Giáº£ láº­p LLM táº¡o cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng káº¿t há»£p text vÃ  áº£nh."""
    print("âœ¨ >> Giáº£ láº­p LLM: Äang táº¡o cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng...")
    final_answer = answer
    
    if image_info:
        final_answer += f"\n\nğŸ–¼ï¸ áº¢nh minh há»a: {image_info.get('caption', 'áº¢nh liÃªn quan')}"
        if image_info.get('image_url'):
            final_answer += f"\nURL: {image_info['image_url']}"
    
    return final_answer

# --- Äá»‹nh nghÄ©a cÃ¡c Node cá»§a Graph ---

def search_context_node(state: GraphState) -> GraphState:
    """Node 1: TÃ¬m kiáº¿m context tá»« vector store."""
    print("\n--- BÆ¯á»šC 1: TÃŒM KIáº¾M CONTEXT ---")
    question = state.get("question")
    
    try:
        # Embed query
        query_embedding = embedding_model.embed_query(question)
        
        # TÃ¬m kiáº¿m context vá»›i score
        results = context_store.similarity_search_with_score(
            query_embedding=query_embedding, 
            k=5
        )
        
        if not results:
            return {**state, "error": "KhÃ´ng tÃ¬m tháº¥y context phÃ¹ há»£p."}
        
        # Chuyá»ƒn Ä‘á»•i káº¿t quáº£ thÃ nh format phÃ¹ há»£p
        text_chunks = []
        for doc, score in results:
            chunk = {
                "content": doc.page_content,
                "metadata": doc.metadata,
                "score": score,
                "chunk_id": doc.metadata.get("chunk_id", str(uuid.uuid4()))
            }
            text_chunks.append(chunk)
        
        print(f"âœ… TÃ¬m tháº¥y {len(text_chunks)} Ä‘oáº¡n context phÃ¹ há»£p")
        return {**state, "text_chunks": text_chunks}
        
    except Exception as e:
        return {**state, "error": f"Lá»—i khi tÃ¬m kiáº¿m context: {str(e)}"}

def generate_answer_node(state: GraphState) -> GraphState:
    """Node 2: Sinh cÃ¢u tráº£ lá»i tá»« context."""
    print("\n--- BÆ¯á»šC 2: SINH CÃ‚U TRáº¢ Lá»œI ---")
    question = state.get("question")
    text_chunks = state.get("text_chunks", [])
    
    if state.get("error"):
        return state
    
    answer = mock_llm_generate_answer(question, text_chunks)
    return {**state, "answer": answer}

def generate_caption_query_node(state: GraphState) -> GraphState:
    """Node 3: Táº¡o query Ä‘á»ƒ tÃ¬m kiáº¿m áº£nh."""
    print("\n--- BÆ¯á»šC 3: Táº O QUERY TÃŒM KIáº¾M áº¢NH ---")
    question = state.get("question")
    answer = state.get("answer")
    
    if state.get("error"):
        return state
    
    caption_query = mock_llm_generate_caption_query(question, answer)
    return {**state, "caption_query": caption_query}

def search_image_node(state: GraphState) -> GraphState:
    """Node 4: TÃ¬m kiáº¿m áº£nh tá»« caption query."""
    print("\n--- BÆ¯á»šC 4: TÃŒM KIáº¾M áº¢NH ---")
    caption_query = state.get("caption_query")
    
    if state.get("error") or not caption_query:
        return state
    
    try:
        # Embed caption query
        query_embedding = embedding_model.embed_query(caption_query)
        
        # TÃ¬m kiáº¿m trong image store
        results = image_store.similarity_search_with_score(
            query_embedding=query_embedding,
            k=1  # Chá»‰ láº¥y 1 áº£nh Ä‘áº¡i diá»‡n
        )
        
        if results:
            doc, score = results[0]
            image_info = {
                "caption": doc.page_content,
                "metadata": doc.metadata,
                "score": score,
                "image_url": doc.metadata.get("image_url"),  # Giáº£ sá»­ metadata chá»©a URL áº£nh
                "image_id": doc.metadata.get("image_id")
            }
            print(f"âœ… TÃ¬m tháº¥y áº£nh phÃ¹ há»£p vá»›i score: {score}")
        else:
            print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y áº£nh phÃ¹ há»£p")
            image_info = None
        
        return {**state, "image_info": image_info}
        
    except Exception as e:
        print(f"âš ï¸ Lá»—i khi tÃ¬m kiáº¿m áº£nh: {str(e)}")
        return {**state, "image_info": None}

def finalize_answer_node(state: GraphState) -> GraphState:
    """Node 5: Táº¡o cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng káº¿t há»£p text vÃ  áº£nh."""
    print("\n--- BÆ¯á»šC 5: HOÃ€N THIá»†N CÃ‚U TRáº¢ Lá»œI ---")
    answer = state.get("answer")
    image_info = state.get("image_info")
    
    if state.get("error"):
        final_answer = f"ÄÃ£ cÃ³ lá»—i xáº£y ra: {state.get('error')}"
    else:
        final_answer = mock_llm_generate_final_answer(answer, image_info)
    
    return {**state, "final_answer": final_answer}

# --- XÃ¢y dá»±ng vÃ  Compile Graph ---

def create_enhanced_rag_graph() -> "CompiledGraph":
    """Táº¡o graph cho RAG pipeline má»›i."""
    workflow = StateGraph(GraphState)

    # ThÃªm cÃ¡c node vÃ o graph
    workflow.add_node("search_context", search_context_node)
    workflow.add_node("generate_answer", generate_answer_node)
    workflow.add_node("generate_caption_query", generate_caption_query_node)
    workflow.add_node("search_image", search_image_node)
    workflow.add_node("finalize_answer", finalize_answer_node)

    # Káº¿t ná»‘i cÃ¡c node theo flow tuáº§n tá»±
    workflow.set_entry_point("search_context")
    workflow.add_edge("search_context", "generate_answer")
    workflow.add_edge("generate_answer", "generate_caption_query")
    workflow.add_edge("generate_caption_query", "search_image")
    workflow.add_edge("search_image", "finalize_answer")
    workflow.add_edge("finalize_answer", END)

    # Compile graph Ä‘á»ƒ sáºµn sÃ ng sá»­ dá»¥ng
    return workflow.compile()

# --- Cháº¡y Pipeline ---

if __name__ == "__main__":
    # Khá»Ÿi táº¡o graph
    app = create_enhanced_rag_graph()

    # Äá»‹nh nghÄ©a cÃ¢u há»i Ä‘áº§u vÃ o
    inputs = {"question": "BÃ¡ch tháº£o sÆ°Æ¡ng dÃ¹ng Ä‘á»ƒ chá»¯a cháº£y mÃ¡u káº½ rÄƒng nhÆ° tháº¿ nÃ o?"}
    
    # Cháº¡y pipeline vÃ  xem cÃ¡c bÆ°á»›c thá»±c thi
    print("ğŸš€ Báº®T Äáº¦U CHáº Y ENHANCED RAG PIPELINE ğŸš€")
    print(f"ğŸ“ CÃ¢u há»i: {inputs['question']}")
    
    for output in app.stream(inputs, stream_mode="values"):
        # `stream_mode="values"` sáº½ tráº£ vá» state sau má»—i bÆ°á»›c
        print("\n" + "="*50)
        print("Tráº¡ng thÃ¡i hiá»‡n táº¡i cá»§a Graph:")
        
        # Hiá»ƒn thá»‹ thÃ´ng tin quan trá»ng
        if output.get("text_chunks"):
            print(f"ğŸ“„ Context chunks: {len(output['text_chunks'])} Ä‘oáº¡n")
        if output.get("answer"):
            print(f"ğŸ’¬ Answer: {output['answer'][:100]}...")
        if output.get("caption_query"):
            print(f"ğŸ” Caption query: {output['caption_query']}")
        if output.get("image_info"):
            print(f"ğŸ–¼ï¸ Image found: {output['image_info'].get('caption', 'N/A')}")
        if output.get("error"):
            print(f"âŒ Error: {output['error']}")
            
        print("="*50)

    print("\nğŸ PIPELINE HOÃ€N Táº¤T! ğŸ")
    print("\nCÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng:")
    print("-" * 50)
    print(output.get("final_answer"))