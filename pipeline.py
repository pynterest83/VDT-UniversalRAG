import uuid
from typing import TypedDict, List, Optional
import os
from dotenv import load_dotenv
from langchain_huggingface import HuggingFaceEmbeddings
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from openai import AzureOpenAI
import json
import tiktoken

load_dotenv()

from langgraph.graph import StateGraph, END, START
from vectorstore import VectorStore

os.environ["LANGCHAIN_TRACING_V2"] = "false"

class GraphState(TypedDict):
    question: str
    initial_chunks: Optional[List[dict]]
    reranked_chunks: Optional[List[dict]]
    selected_contexts: Optional[List[dict]]
    answer: Optional[str]
    image_info: Optional[dict]
    final_answer: Optional[str]
    error: Optional[str]


# --- Khá»Ÿi táº¡o cÃ¡c thÃ nh pháº§n ---
def initialize_components():
    context_embedding_model = HuggingFaceEmbeddings(model_name="./bge-m3-v3")
    
    reranker_model_path = "AITeamVN/Vietnamese_Reranker"
    reranker_tokenizer = AutoTokenizer.from_pretrained(reranker_model_path, use_fast=False)
    reranker_model = AutoModelForSequenceClassification.from_pretrained(
        reranker_model_path,
        torch_dtype=torch.float16,
        device_map="auto" if torch.cuda.is_available() else None
    )
    reranker_model.eval()
    
    image_embedding_model = HuggingFaceEmbeddings(model_name="./bge-m3-image")
    
    azure_client = AzureOpenAI(
        azure_endpoint=os.getenv("AZURE_ENDPOINT"),
        api_key=os.getenv("AZURE_API_KEY"),
        api_version=os.getenv("AZURE_VERSION")
    )
    
    context_store = VectorStore(collection_name="universal-rag-precomputed-enhanced")
    image_store = VectorStore(collection_name="image-captions-store")
    
    return context_embedding_model, reranker_tokenizer, reranker_model, image_embedding_model, azure_client, context_store, image_store

context_embedding_model, reranker_tokenizer, reranker_model, image_embedding_model, azure_client, context_store, image_store = initialize_components()
MAX_LENGTH = 2304

# Initialize tiktoken encoder for GPT-4o
encoding = tiktoken.encoding_for_model("gpt-4o")

def rerank_documents(query: str, documents: List[dict], top_k: int = 5) -> List[dict]:
    if len(documents) <= top_k:
        return documents
    
    doc_contents = [doc['content'] for doc in documents]
    pairs = [[query, doc_content] for doc_content in doc_contents]
    
    with torch.no_grad():
        inputs = reranker_tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=MAX_LENGTH)
        if torch.cuda.is_available() and reranker_model.device.type == 'cuda':
            inputs = {k: v.to(reranker_model.device) for k, v in inputs.items()}
        scores = reranker_model(**inputs, return_dict=True).logits.view(-1, ).float()
    
    document_scores = list(zip(documents, scores.cpu().numpy()))
    document_scores.sort(key=lambda x: x[1], reverse=True)
    
    reranked_docs = []
    for i, (doc, score) in enumerate(document_scores[:top_k]):
        doc['rerank_score'] = float(score)
        reranked_docs.append(doc)
    
    return reranked_docs

def llm_select_context(question: str, ranked_chunks: List[dict]) -> List[dict]:
    
    if not ranked_chunks:
        return []
    
    try:
        context_list = []
        total_context_length = 0
        
        for i, chunk in enumerate(ranked_chunks):
            content = chunk['content']
            total_context_length += len(content)
            context_list.append({
                "priority_order": i + 1,
                "rerank_score": chunk.get('rerank_score', 0),
                "content": content,
                "chunk_id": chunk.get('chunk_id', f"chunk_{i+1}"),
            })
        
        max_context_tokens = max(len(encoding.encode(chunk['content'])) for chunk in ranked_chunks) if ranked_chunks else 0
        max_tokens = max_context_tokens + 200
        
        prompt = f"""
# NHIá»†M Vá»¤: PHÃ‚N TÃCH VÃ€ Lá»°A CHá»ŒN CONTEXT Tá»I Æ¯U

## Bá»I Cáº¢NH
Báº¡n lÃ  má»™t chuyÃªn gia phÃ¢n tÃ­ch thÃ´ng tin y táº¿ vá»›i chuyÃªn mÃ´n sÃ¢u vá»:
- Y há»c lÃ¢m sÃ ng vÃ  cháº©n Ä‘oÃ¡n bá»‡nh
- DÆ°á»£c há»c vÃ  tÃ¡c dá»¥ng cá»§a thuá»‘c
- Tháº£o dÆ°á»£c vÃ  y há»c tá»± nhiÃªn  
- Sinh lÃ½ bá»‡nh vÃ  cÆ¡ cháº¿ bá»‡nh táº­t
- Äiá»u trá»‹ vÃ  phÃ²ng ngá»«a bá»‡nh

## NHIá»†M Vá»¤ Cá»¤ THá»‚
PhÃ¢n tÃ­ch danh sÃ¡ch context Ä‘Ã£ Ä‘Æ°á»£c sáº¯p xáº¿p theo Ä‘á»™ liÃªn quan (tá»« mÃ´ hÃ¬nh reranking) vÃ  lá»±a chá»n context tá»‘i Æ°u nháº¥t Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i y táº¿.

## NGUYÃŠN Táº®C Lá»°A CHá»ŒN
1. **Äá»™ chÃ­nh xÃ¡c**: Context pháº£i chá»©a thÃ´ng tin chÃ­nh xÃ¡c, khoa há»c vá» chá»§ Ä‘á» Ä‘Æ°á»£c há»i
2. **Äá»™ Ä‘áº§y Ä‘á»§**: Context pháº£i cung cáº¥p Ä‘á»§ thÃ´ng tin Ä‘á»ƒ tráº£ lá»i hoÃ n chá»‰nh cÃ¢u há»i
3. **Äá»™ tin cáº­y**: Æ¯u tiÃªn context tá»« nguá»“n uy tÃ­n, cÃ³ cÆ¡ sá»Ÿ khoa há»c
4. **TÃ­nh cá»¥ thá»ƒ**: Context pháº£i cá»¥ thá»ƒ vá» cÆ¡ cháº¿, liá»u lÆ°á»£ng, cÃ¡ch sá»­ dá»¥ng
5. **Tá»‘i Æ°u sá»‘ lÆ°á»£ng**: Æ¯u tiÃªn chá»n 1 context Ä‘áº§y Ä‘á»§, chá»‰ láº¥y thÃªm náº¿u thá»±c sá»± cáº§n thiáº¿t

## THÃ”NG TIN CONTEXT
- Táº¥t cáº£ context Ä‘Æ°á»£c hiá»ƒn thá»‹ Ä‘áº§y Ä‘á»§ (khÃ´ng bá»‹ cáº¯t bá»›t)
- Field "content_length" cho biáº¿t Ä‘á»™ dÃ i cá»§a tá»«ng context
- CÃ³ thá»ƒ phÃ¢n tÃ­ch toÃ n bá»™ ná»™i dung Ä‘á»ƒ Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh chÃ­nh xÃ¡c

## Dá»® LIá»†U Äáº¦U VÃ€O

**CÃ‚U Há»ŽI Y Táº¾:**
{question}

**DANH SÃCH CONTEXT (theo thá»© tá»± Æ°u tiÃªn tá»« reranking model):**
{json.dumps(context_list, ensure_ascii=False, indent=2)}

## YÃŠU Cáº¦U PHÃ‚N TÃCH

### BÆ¯á»šC 1: ÄÃ¡nh giÃ¡ tá»«ng context
- XÃ¡c Ä‘á»‹nh má»©c Ä‘á»™ liÃªn quan trá»±c tiáº¿p Ä‘áº¿n cÃ¢u há»i (0-10)
- ÄÃ¡nh giÃ¡ Ä‘á»™ Ä‘áº§y Ä‘á»§ thÃ´ng tin (cÃ³ Ä‘á»§ Ä‘á»ƒ tráº£ lá»i khÃ´ng?)
- Kiá»ƒm tra tÃ­nh chÃ­nh xÃ¡c vÃ  cÆ¡ sá»Ÿ khoa há»c
- XÃ¡c Ä‘á»‹nh Ä‘iá»ƒm máº¡nh vÃ  Ä‘iá»ƒm yáº¿u cá»§a má»—i context

### BÆ¯á»šC 2: Lá»±a chá»n context tá»‘i Æ°u
- Chá»n context cÃ³ Ä‘iá»ƒm tá»•ng há»£p cao nháº¥t (khÃ´ng nháº¥t thiáº¿t pháº£i top 1)
- Quyáº¿t Ä‘á»‹nh cÃ³ cáº§n káº¿t há»£p thÃªm context khÃ¡c khÃ´ng
- Æ¯u tiÃªn giáº£i phÃ¡p tá»‘i thiá»ƒu (1 context náº¿u Ä‘á»§)

### BÆ¯á»šC 3: ÄÆ°a ra quyáº¿t Ä‘á»‹nh cuá»‘i cÃ¹ng

## FORMAT TRáº¢ Lá»œI
Tráº£ vá» ÄÃšNG format JSON sau (khÃ´ng thÃªm text nÃ o khÃ¡c):

{{
    "selected_chunk_ids": ["chunk_X", "chunk_Y"],
    "reasoning": "PhÃ¢n tÃ­ch chi tiáº¿t: Context chunk_X Ä‘Æ°á»£c chá»n vÃ¬ [lÃ½ do cá»¥ thá»ƒ vá» Ä‘á»™ chÃ­nh xÃ¡c, Ä‘áº§y Ä‘á»§, tin cáº­y]. [Náº¿u chá»n thÃªm context khÃ¡c thÃ¬ giáº£i thÃ­ch táº¡i sao cáº§n thiáº¿t]",
    "confidence": 0.XX,
    "analysis": {{
        "primary_context": "chunk_X",
        "supplementary_contexts": ["chunk_Y"] hoáº·c [],
        "coverage_assessment": "ÄÃ¡nh giÃ¡ má»©c Ä‘á»™ Ä‘á»§ thÃ´ng tin Ä‘á»ƒ tráº£ lá»i (Ä‘áº§y Ä‘á»§/má»™t pháº§n/khÃ´ng Ä‘á»§)"
    }}
}}

**LÆ¯U Ã QUAN TRá»ŒNG:**
- CHá»ˆ tráº£ vá» JSON, khÃ´ng cÃ³ text giáº£i thÃ­ch thÃªm
- Confidence score pháº£i pháº£n Ã¡nh chÃ­nh xÃ¡c má»©c Ä‘á»™ tin tÆ°á»Ÿng vÃ o lá»±a chá»n
- Reasoning pháº£i cá»¥ thá»ƒ vÃ  cÃ³ cÄƒn cá»© khoa há»c
"""
        
        response = azure_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "Báº¡n lÃ  má»™t chuyÃªn gia phÃ¢n tÃ­ch vÄƒn báº£n vÃ  lá»±a chá»n context."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
            max_tokens=max_tokens,
            response_format={"type": "json_object"}
        )
        
        result_text = response.choices[0].message.content.strip()
        
        try:
            result = json.loads(result_text)
            selected_chunk_ids = result.get("selected_chunk_ids", [])
            
            selected_contexts = []
            for chunk_id in selected_chunk_ids:
                for chunk in ranked_chunks:
                    if chunk.get('chunk_id') == chunk_id:
                        selected_contexts.append(chunk)
                        break
            
            return selected_contexts
            
        except json.JSONDecodeError:
            return [ranked_chunks[0]]
        
    except Exception as e:
        return [ranked_chunks[0]] if ranked_chunks else []

def llm_generate_answer(question: str, selected_contexts: List[dict]) -> str:
    if not selected_contexts:
        return "Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i nÃ y."
    
    try:
        context_text = "\n\n".join([f"Context {i+1}:\n{ctx['content']}" for i, ctx in enumerate(selected_contexts)])
        
        prompt = f"""
# NHIá»†M Vá»¤: TRáº¢ Lá»œI CÃ‚U Há»ŽI Y Táº¾ Dá»°A TRÃŠN CONTEXT

## VAI TRÃ’ VÃ€ CHUYÃŠN MÃ”N
Báº¡n lÃ  má»™t chuyÃªn gia y táº¿ Ä‘a lÄ©nh vá»±c vá»›i kiáº¿n thá»©c sÃ¢u vá»:
- **Y há»c lÃ¢m sÃ ng**: Cháº©n Ä‘oÃ¡n, Ä‘iá»u trá»‹, theo dÃµi bá»‡nh nhÃ¢n
- **DÆ°á»£c há»c**: CÆ¡ cháº¿ tÃ¡c dá»¥ng, tÆ°Æ¡ng tÃ¡c thuá»‘c, liá»u lÆ°á»£ng, tÃ¡c dá»¥ng phá»¥
- **Tháº£o dÆ°á»£c**: ThÃ nh pháº§n hoáº¡t tÃ­nh, cÃ´ng dá»¥ng, cÃ¡ch cháº¿ biáº¿n vÃ  sá»­ dá»¥ng
- **Sinh lÃ½ bá»‡nh**: CÆ¡ cháº¿ phÃ¡t sinh vÃ  tiáº¿n triá»ƒn bá»‡nh
- **Y há»c phÃ²ng chá»‘ng**: Biá»‡n phÃ¡p phÃ²ng ngá»«a vÃ  chÄƒm sÃ³c sá»©c khá»e
- **Y há»c cá»• truyá»n**: PhÆ°Æ¡ng phÃ¡p Ä‘iá»u trá»‹ truyá»n thá»‘ng cÃ³ cÆ¡ sá»Ÿ khoa há»c

## NGUYÃŠN Táº®C TRáº¢ Lá»œI
1. **ChÃ­nh xÃ¡c khoa há»c**: ThÃ´ng tin pháº£i cÃ³ cÆ¡ sá»Ÿ khoa há»c rÃµ rÃ ng
2. **Dá»±a trÃªn context**: Chá»‰ sá»­ dá»¥ng thÃ´ng tin cÃ³ trong context Ä‘Æ°á»£c cung cáº¥p
3. **NGáº®N Gá»ŒN Tá»I ÄA**: 20-80 tá»«, Æ°u tiÃªn 30-50 tá»«
4. **Trá»±c tiáº¿p**: Tráº£ lá»i tháº³ng vÃ o váº¥n Ä‘á», khÃ´ng dÃ i dÃ²ng
5. **Thá»±c tiá»…n**: Cung cáº¥p thÃ´ng tin cá»‘t lÃµi nháº¥t

## Dá»® LIá»†U Äáº¦U VÃ€O

**CÃ‚U Há»ŽI Cáº¦N TRáº¢ Lá»œI:**
{question}

**CONTEXT THAM KHáº¢O:**
{context_text}

## YÃŠU Cáº¦U Cá»¤ THá»‚

### Cáº¥u trÃºc cÃ¢u tráº£ lá»i (NGáº®N Gá»ŒN):
1. **Tráº£ lá»i trá»±c tiáº¿p** trong 1-2 cÃ¢u chÃ­nh
2. **ThÃ´ng tin cá»‘t lÃµi** (cÆ¡ cháº¿/liá»u lÆ°á»£ng/cÃ¡ch dÃ¹ng) náº¿u cÃ³ trong context
3. **LÆ°u Ã½ quan trá»ng** (náº¿u cáº§n thiáº¿t)

### TiÃªu chuáº©n cháº¥t lÆ°á»£ng:
- **Äá»™ dÃ i**: KhÃ´ng quÃ¡ dÃ i (tá»‘i Æ°u 30-50 tá»«)
- **NgÃ´n ngá»¯**: Tiáº¿ng Viá»‡t sÃºc tÃ­ch, khoa há»c
- **Cáº¥u trÃºc**: Trá»±c tiáº¿p, khÃ´ng giáº£i thÃ­ch dÃ i dÃ²ng
- **Ná»™i dung**: Chá»‰ thÃ´ng tin thiáº¿t yáº¿u nháº¥t

### LÆ°u Ã½ Ä‘áº·c biá»‡t:
- KHÃ”NG bá»‹a Ä‘áº·t thÃ´ng tin khÃ´ng cÃ³ trong context
- KHÃ”NG giáº£i thÃ­ch chi tiáº¿t náº¿u khÃ´ng cáº§n thiáº¿t
- Æ¯u tiÃªn thÃ´ng tin thá»±c tiá»…n, cá»¥ thá»ƒ
- Sá»­ dá»¥ng "theo tÃ i liá»‡u" khi cáº§n thiáº¿t

## TRáº¢ Lá»œI (NGáº®N Gá»ŒN):
"""
        
        response = azure_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "Báº¡n lÃ  chuyÃªn gia y táº¿ chuyÃªn tráº£ lá»i Cá»°C NGáº®N Gá»ŒN vÃ  CHÃNH XÃC. Chá»‰ nÃ³i nhá»¯ng gÃ¬ cáº§n thiáº¿t nháº¥t."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
            max_tokens=300
        )
        
        answer = response.choices[0].message.content.strip()
        return answer
        
    except Exception as e:
        context_preview = selected_contexts[0]['content'][:300] if selected_contexts else ""
        return f"Dá»±a trÃªn thÃ´ng tin tÃ¬m Ä‘Æ°á»£c: {context_preview}..."


def search_initial_context_node(state: GraphState) -> GraphState:
    question = state.get("question")
    
    try:
        query_embedding = context_embedding_model.embed_query(question)
        results = context_store.similarity_search_with_score(query_embedding=query_embedding, k=10)
        
        if not results:
            return {**state, "error": "KhÃ´ng tÃ¬m tháº¥y context phÃ¹ há»£p."}
        
        initial_chunks = []
        for doc, score in results:
            chunk = {
                "content": doc.page_content,
                "metadata": doc.metadata,
                "score": score,
                "chunk_id": doc.metadata.get("chunk_id", str(uuid.uuid4()))
            }
            initial_chunks.append(chunk)
        
        return {**state, "initial_chunks": initial_chunks}
        
    except Exception as e:
        return {**state, "error": f"Lá»—i khi tÃ¬m kiáº¿m context: {str(e)}"}

def rerank_context_node(state: GraphState) -> GraphState:
    question = state.get("question")
    initial_chunks = state.get("initial_chunks", [])
    
    if state.get("error") or not initial_chunks:
        return state
    
    try:
        reranked_chunks = rerank_documents(question, initial_chunks, top_k=5)
        return {**state, "reranked_chunks": reranked_chunks}
        
    except Exception as e:
        return {**state, "error": f"Lá»—i khi rerank context: {str(e)}"}

def select_context_node(state: GraphState) -> GraphState:
    question = state.get("question")
    reranked_chunks = state.get("reranked_chunks", [])
    
    if state.get("error") or not reranked_chunks:
        return state
    
    try:
        selected_contexts = llm_select_context(question, reranked_chunks)
        return {**state, "selected_contexts": selected_contexts}
        
    except Exception as e:
        return {**state, "error": f"Lá»—i khi lá»±a chá»n context: {str(e)}"}

def generate_answer_node(state: GraphState) -> GraphState:
    question = state.get("question")
    selected_contexts = state.get("selected_contexts", [])
    
    if state.get("error"):
        return state
    
    answer = llm_generate_answer(question, selected_contexts)
    return {**state, "answer": answer}

def search_image_node(state: GraphState) -> GraphState:
    selected_contexts = state.get("selected_contexts", [])
    
    if state.get("error") or not selected_contexts:
        return state
    
    try:
        main_context = selected_contexts[0]['content']
        context_embedding = image_embedding_model.embed_query(main_context)
        results = image_store.similarity_search_with_score(query_embedding=context_embedding, k=1)
        
        if results:
            doc, score = results[0]
            image_info = {
                "caption": doc.page_content,
                "metadata": doc.metadata,
                "score": score,
                "image_name": doc.metadata.get("image_name"),
                "image_path": doc.metadata.get("image_path"), 
                "image_url": doc.metadata.get("image_url"),
                "source": doc.metadata.get("source"),
                "image_id": doc.metadata.get("image_id")
            }
        else:
            image_info = None
        
        return {**state, "image_info": image_info}
        
    except Exception as e:
        return {**state, "image_info": None}

def finalize_answer_node(state: GraphState) -> GraphState:
    answer = state.get("answer")
    image_info = state.get("image_info")
    
    if state.get("error"):
        final_answer = f"ÄÃ£ cÃ³ lá»—i xáº£y ra: {state.get('error')}"
    else:
        final_answer = answer
        
        if image_info:
            final_answer += f"\n\nðŸ–¼ï¸ áº¢nh minh há»a: {image_info.get('caption', 'áº¢nh liÃªn quan')}"
            
            if image_info.get('image_name'):
                final_answer += f"\nTÃªn áº£nh: {image_info['image_name']}"
            if image_info.get('image_path'):
                final_answer += f"\nÄÆ°á»ng dáº«n: {image_info['image_path']}"
            if image_info.get('source'):
                final_answer += f"\nNguá»“n: {image_info['source']}"
            if image_info.get('image_url'):
                final_answer += f"\nURL: {image_info['image_url']}"
    
    return {**state, "final_answer": final_answer}

def create_enhanced_rag_graph() -> "CompiledGraph":
    workflow = StateGraph(GraphState)

    workflow.add_node("search_initial_context", search_initial_context_node)
    workflow.add_node("rerank_context", rerank_context_node)
    workflow.add_node("select_context", select_context_node)
    workflow.add_node("generate_answer", generate_answer_node)
    workflow.add_node("search_image", search_image_node)
    workflow.add_node("finalize_answer", finalize_answer_node)

    workflow.set_entry_point("search_initial_context")
    workflow.add_edge("search_initial_context", "rerank_context")
    workflow.add_edge("rerank_context", "select_context")
    workflow.add_edge("select_context", "generate_answer")
    workflow.add_edge("generate_answer", "search_image")
    workflow.add_edge("search_image", "finalize_answer")
    workflow.add_edge("finalize_answer", END)

    return workflow.compile()