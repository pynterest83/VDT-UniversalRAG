import uuid
from typing import TypedDict, List, Optional
import os
from dotenv import load_dotenv
import requests
import json
import tiktoken
from openai import AzureOpenAI
import base64
from PIL import Image
from io import BytesIO

load_dotenv()

from langgraph.graph import StateGraph, END, START
from vectorstore import VectorStore

os.environ["LANGCHAIN_TRACING_V2"] = "false"

class GraphState(TypedDict):
    question: str
    initial_chunks: Optional[List[dict]]
    reranked_chunks: Optional[List[dict]]
    selected_contexts: Optional[List[dict]]
    answer: Optional[str]
    available_images: Optional[List[dict]]
    selected_image: Optional[dict]
    generated_image: Optional[dict]
    final_answer: Optional[str]
    error: Optional[str]
    user_choice: Optional[str]

MODAL_BASE_URL = "https://ise703--universal-rag-models"
MODAL_ENDPOINTS = {
    "context_embedding": f"{MODAL_BASE_URL}-embed-context.modal.run",
    "image_embedding": f"{MODAL_BASE_URL}-embed-image-caption.modal.run",
    "reranker": f"{MODAL_BASE_URL}-rerank-documents.modal.run"
}

def initialize_components():
    azure_client = AzureOpenAI(
        azure_endpoint=os.getenv("AZURE_ENDPOINT"),
        api_key=os.getenv("AZURE_API_KEY"),
        api_version=os.getenv("AZURE_VERSION")
    )

    image_client = AzureOpenAI(
        api_key=os.getenv("AZURE_IMAGE_API_KEY"),
        api_version=os.getenv("AZURE_IMAGE_VERSION"),
        azure_endpoint=os.getenv("AZURE_IMAGE_ENDPOINT")
    )
    
    context_store = VectorStore(collection_name="universal-rag-precomputed-enhanced")
    image_store = VectorStore(collection_name="image-captions-store")
    
    return azure_client, image_client, context_store, image_store

azure_client, image_client, context_store, image_store = initialize_components()
encoding = tiktoken.encoding_for_model("gpt-4o")

def create_image_prompt(question: str, context: str, answer: str):
    return f"""
Generate a realistic, high-quality, professional medical image that visually represents the following information. The image should be suitable for a medical consultation or educational material.

**Main Answer to Illustrate:**
{answer}

**Full Context for Detail:**
- **User's Question:** {question}
- **Supporting Information:** {context}

**Guidelines:**
- **Style**: Realistic photography. Avoid illustrations, cartoons, or abstract art.
- **Focus**: The main goal is to visually represent the **Main Answer**. Use the question and supporting information to add accurate details.
- **Clarity**: The image must be clear, professional, and easy to understand.
- **Setting**: A clean, modern medical environment like a clinic, hospital, or pharmacy.
- **Important**: Do not include any visible text, logos, or branding in the image. The focus is on the visual representation of the medical information.
- **Tone**: Professional, trustworthy, and informative.

Create an image that effectively visualizes the provided medical answer, informed by the original question and context.
"""

def generate_medical_image(question: str, context: str, answer: str):
    prompt = create_image_prompt(question, context, answer)
    os.makedirs("imgs", exist_ok=True)
    
    result = image_client.images.generate(
        model="gpt-image-1",
        prompt=prompt,
        size="1024x1024"
    )
    
    image_base64 = result.data[0].b64_json
    image_bytes = base64.b64decode(image_base64)
    image = Image.open(BytesIO(image_bytes))
    image = image.resize((512, 512), Image.LANCZOS)
    
    import time
    timestamp = int(time.time())
    filename = f"generated_{timestamp}.jpg"
    full_path = f"imgs/{filename}"
    
    image.save(full_path, format="JPEG", quality=95, optimize=True)
    
    return {
        "image_path": full_path,
        "image_name": filename,
        "caption": f"·∫¢nh ƒë∆∞·ª£c sinh t·ª± ƒë·ªông cho c√¢u h·ªèi: {question}",
        "source": "AI Generated",
        "score": 1.0,
        "type": "generated"
    }

def get_context_embedding(text: str) -> List[float]:
    response = requests.post(
        MODAL_ENDPOINTS["context_embedding"],
        json={"text": text},
        timeout=120
    )
    response.raise_for_status()
    result = response.json()
    return result["embedding"]

def get_image_embedding(text: str) -> List[float]:
    response = requests.post(
        MODAL_ENDPOINTS["image_embedding"],
        json={"text": text},
        timeout=120
    )
    response.raise_for_status()
    result = response.json()
    return result["embedding"]

def rerank_documents_api(query: str, documents: List[dict], top_k: int = 5) -> List[dict]:
    try:
        response = requests.post(
            MODAL_ENDPOINTS["reranker"],
            json={
                "query": query,
                "documents": documents,
                "top_k": top_k
            },
            timeout=240
        )
        response.raise_for_status()
        result = response.json()
        
        if "reranked_documents" in result and result["reranked_documents"]:
            return result["reranked_documents"]
        
        raise Exception("No valid reranked documents returned")
        
    except Exception as e:
        print(f"Reranker API error, falling back: {str(e)}")
        fallback_docs = []
        for i, doc in enumerate(documents[:top_k]):
            doc_copy = doc.copy()
            doc_copy['rerank_score'] = 1.0 - (i * 0.1)
            fallback_docs.append(doc_copy)
        return fallback_docs

def llm_select_context(question: str, ranked_chunks: List[dict]) -> List[dict]:
    
    if not ranked_chunks:
        return []
    
    try:
        context_list = []
        total_context_length = 0
        
        for i, chunk in enumerate(ranked_chunks):
            content = chunk['content']
            total_context_length += len(content)
            context_list.append({
                "priority_order": i + 1,
                "rerank_score": chunk.get('rerank_score', 0),
                "content": content,
                "chunk_id": chunk.get('chunk_id', f"chunk_{i+1}"),
            })
        
        max_context_tokens = max(len(encoding.encode(chunk['content'])) for chunk in ranked_chunks) if ranked_chunks else 0
        max_tokens = max_context_tokens + 200
        
        prompt = f"""
# NHI·ªÜM V·ª§: PH√ÇN T√çCH V√Ä L·ª∞A CH·ªåN CONTEXT T·ªêI ∆ØU

## B·ªêI C·∫¢NH
B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch th√¥ng tin y t·∫ø v·ªõi chuy√™n m√¥n s√¢u v·ªÅ:
- Y h·ªçc l√¢m s√†ng v√† ch·∫©n ƒëo√°n b·ªánh
- D∆∞·ª£c h·ªçc v√† t√°c d·ª•ng c·ªßa thu·ªëc
- Th·∫£o d∆∞·ª£c v√† y h·ªçc t·ª± nhi√™n  
- Sinh l√Ω b·ªánh v√† c∆° ch·∫ø b·ªánh t·∫≠t
- ƒêi·ªÅu tr·ªã v√† ph√≤ng ng·ª´a b·ªánh

## NHI·ªÜM V·ª§ C·ª§ TH·ªÇ
Ph√¢n t√≠ch danh s√°ch context ƒë√£ ƒë∆∞·ª£c s·∫Øp x·∫øp theo ƒë·ªô li√™n quan (t·ª´ m√¥ h√¨nh reranking) v√† l·ª±a ch·ªçn context t·ªëi ∆∞u nh·∫•t ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi y t·∫ø.

## NGUY√äN T·∫ÆC L·ª∞A CH·ªåN
1. **ƒê·ªô ch√≠nh x√°c**: Context ph·∫£i ch·ª©a th√¥ng tin ch√≠nh x√°c, khoa h·ªçc v·ªÅ ch·ªß ƒë·ªÅ ƒë∆∞·ª£c h·ªèi
2. **ƒê·ªô ƒë·∫ßy ƒë·ªß**: Context ph·∫£i cung c·∫•p ƒë·ªß th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi ho√†n ch·ªânh c√¢u h·ªèi
3. **ƒê·ªô tin c·∫≠y**: ∆Øu ti√™n context t·ª´ ngu·ªìn uy t√≠n, c√≥ c∆° s·ªü khoa h·ªçc
4. **T√≠nh c·ª• th·ªÉ**: Context ph·∫£i c·ª• th·ªÉ v·ªÅ c∆° ch·∫ø, li·ªÅu l∆∞·ª£ng, c√°ch s·ª≠ d·ª•ng
5. **T·ªëi ∆∞u s·ªë l∆∞·ª£ng**: ∆Øu ti√™n ch·ªçn 1 context ƒë·∫ßy ƒë·ªß, ch·ªâ l·∫•y th√™m n·∫øu th·ª±c s·ª± c·∫ßn thi·∫øt

## TH√îNG TIN CONTEXT
- T·∫•t c·∫£ context ƒë∆∞·ª£c hi·ªÉn th·ªã ƒë·∫ßy ƒë·ªß (kh√¥ng b·ªã c·∫Øt b·ªõt)
- Field "content_length" cho bi·∫øt ƒë·ªô d√†i c·ªßa t·ª´ng context
- C√≥ th·ªÉ ph√¢n t√≠ch to√†n b·ªô n·ªôi dung ƒë·ªÉ ƒë∆∞a ra quy·∫øt ƒë·ªãnh ch√≠nh x√°c

## D·ªÆ LI·ªÜU ƒê·∫¶U V√ÄO

**C√ÇU H·ªéI Y T·∫æ:**
{question}

**DANH S√ÅCH CONTEXT (theo th·ª© t·ª± ∆∞u ti√™n t·ª´ reranking model):**
{json.dumps(context_list, ensure_ascii=False, indent=2)}

## Y√äU C·∫¶U PH√ÇN T√çCH

### B∆Ø·ªöC 1: ƒê√°nh gi√° t·ª´ng context
- X√°c ƒë·ªãnh m·ª©c ƒë·ªô li√™n quan tr·ª±c ti·∫øp ƒë·∫øn c√¢u h·ªèi (0-10)
- ƒê√°nh gi√° ƒë·ªô ƒë·∫ßy ƒë·ªß th√¥ng tin (c√≥ ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi kh√¥ng?)
- Ki·ªÉm tra t√≠nh ch√≠nh x√°c v√† c∆° s·ªü khoa h·ªçc
- X√°c ƒë·ªãnh ƒëi·ªÉm m·∫°nh v√† ƒëi·ªÉm y·∫øu c·ªßa m·ªói context

### B∆Ø·ªöC 2: L·ª±a ch·ªçn context t·ªëi ∆∞u
- Ch·ªçn context c√≥ ƒëi·ªÉm t·ªïng h·ª£p cao nh·∫•t (kh√¥ng nh·∫•t thi·∫øt ph·∫£i top 1)
- Quy·∫øt ƒë·ªãnh c√≥ c·∫ßn k·∫øt h·ª£p th√™m context kh√°c kh√¥ng
- ∆Øu ti√™n gi·∫£i ph√°p t·ªëi thi·ªÉu (1 context n·∫øu ƒë·ªß)

### B∆Ø·ªöC 3: ƒê∆∞a ra quy·∫øt ƒë·ªãnh cu·ªëi c√πng

## FORMAT TR·∫¢ L·ªúI
Tr·∫£ v·ªÅ ƒê√öNG format JSON sau (kh√¥ng th√™m text n√†o kh√°c):

{{
    "selected_chunk_ids": ["chunk_X", "chunk_Y"],
    "reasoning": "Ph√¢n t√≠ch chi ti·∫øt: Context chunk_X ƒë∆∞·ª£c ch·ªçn v√¨ [l√Ω do c·ª• th·ªÉ v·ªÅ ƒë·ªô ch√≠nh x√°c, ƒë·∫ßy ƒë·ªß, tin c·∫≠y]. [N·∫øu ch·ªçn th√™m context kh√°c th√¨ gi·∫£i th√≠ch t·∫°i sao c·∫ßn thi·∫øt]",
    "confidence": 0.XX,
    "analysis": {{
        "primary_context": "chunk_X",
        "supplementary_contexts": ["chunk_Y"] ho·∫∑c [],
        "coverage_assessment": "ƒê√°nh gi√° m·ª©c ƒë·ªô ƒë·ªß th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi (ƒë·∫ßy ƒë·ªß/m·ªôt ph·∫ßn/kh√¥ng ƒë·ªß)"
    }}
}}

**L∆ØU √ù QUAN TR·ªåNG:**
- CH·ªà tr·∫£ v·ªÅ JSON, kh√¥ng c√≥ text gi·∫£i th√≠ch th√™m
- Confidence score ph·∫£i ph·∫£n √°nh ch√≠nh x√°c m·ª©c ƒë·ªô tin t∆∞·ªüng v√†o l·ª±a ch·ªçn
- Reasoning ph·∫£i c·ª• th·ªÉ v√† c√≥ cƒÉn c·ª© khoa h·ªçc
"""
        
        response = azure_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch vƒÉn b·∫£n v√† l·ª±a ch·ªçn context."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
            max_tokens=max_tokens,
            response_format={"type": "json_object"}
        )
        
        result_text = response.choices[0].message.content.strip()
        
        try:
            result = json.loads(result_text)
            selected_chunk_ids = result.get("selected_chunk_ids", [])
            
            selected_contexts = []
            for chunk_id in selected_chunk_ids:
                for chunk in ranked_chunks:
                    if chunk.get('chunk_id') == chunk_id:
                        selected_contexts.append(chunk)
                        break
            
            return selected_contexts
            
        except json.JSONDecodeError:
            return [ranked_chunks[0]]
        
    except Exception as e:
        return [ranked_chunks[0]] if ranked_chunks else []

def llm_generate_answer(question: str, selected_contexts: List[dict]) -> str:
    if not selected_contexts:
        return "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."
    
    try:
        context_text = "\n\n".join([f"Context {i+1}:\n{ctx['content']}" for i, ctx in enumerate(selected_contexts)])
        
        prompt = f"""
# NHI·ªÜM V·ª§: TR·∫¢ L·ªúI C√ÇU H·ªéI Y T·∫æ D·ª∞A TR√äN CONTEXT

## VAI TR√í V√Ä CHUY√äN M√îN
B·∫°n l√† m·ªôt chuy√™n gia y t·∫ø ƒëa lƒ©nh v·ª±c v·ªõi ki·∫øn th·ª©c s√¢u v·ªÅ:
- **Y h·ªçc l√¢m s√†ng**: Ch·∫©n ƒëo√°n, ƒëi·ªÅu tr·ªã, theo d√µi b·ªánh nh√¢n
- **D∆∞·ª£c h·ªçc**: C∆° ch·∫ø t√°c d·ª•ng, t∆∞∆°ng t√°c thu·ªëc, li·ªÅu l∆∞·ª£ng, t√°c d·ª•ng ph·ª•
- **Th·∫£o d∆∞·ª£c**: Th√†nh ph·∫ßn ho·∫°t t√≠nh, c√¥ng d·ª•ng, c√°ch ch·∫ø bi·∫øn v√† s·ª≠ d·ª•ng
- **Sinh l√Ω b·ªánh**: C∆° ch·∫ø ph√°t sinh v√† ti·∫øn tri·ªÉn b·ªánh
- **Y h·ªçc ph√≤ng ch·ªëng**: Bi·ªán ph√°p ph√≤ng ng·ª´a v√† chƒÉm s√≥c s·ª©c kh·ªèe
- **Y h·ªçc c·ªï truy·ªÅn**: Ph∆∞∆°ng ph√°p ƒëi·ªÅu tr·ªã truy·ªÅn th·ªëng c√≥ c∆° s·ªü khoa h·ªçc

## NGUY√äN T·∫ÆC TR·∫¢ L·ªúI
1. **Ch√≠nh x√°c khoa h·ªçc**: Th√¥ng tin ph·∫£i c√≥ c∆° s·ªü khoa h·ªçc r√µ r√†ng
2. **D·ª±a tr√™n context**: Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin c√≥ trong context ƒë∆∞·ª£c cung c·∫•p
3. **NG·∫ÆN G·ªåN T·ªêI ƒêA**: 20-80 t·ª´, ∆∞u ti√™n 30-50 t·ª´
4. **Tr·ª±c ti·∫øp**: Tr·∫£ l·ªùi th·∫≥ng v√†o v·∫•n ƒë·ªÅ, kh√¥ng d√†i d√≤ng
5. **Th·ª±c ti·ªÖn**: Cung c·∫•p th√¥ng tin c·ªët l√µi nh·∫•t

## D·ªÆ LI·ªÜU ƒê·∫¶U V√ÄO

**C√ÇU H·ªéI C·∫¶N TR·∫¢ L·ªúI:**
{question}

**CONTEXT THAM KH·∫¢O:**
{context_text}

## Y√äU C·∫¶U C·ª§ TH·ªÇ

### C·∫•u tr√∫c c√¢u tr·∫£ l·ªùi (NG·∫ÆN G·ªåN):
1. **Tr·∫£ l·ªùi tr·ª±c ti·∫øp** trong 1-2 c√¢u ch√≠nh
2. **Th√¥ng tin c·ªët l√µi** (c∆° ch·∫ø/li·ªÅu l∆∞·ª£ng/c√°ch d√πng) n·∫øu c√≥ trong context
3. **L∆∞u √Ω quan tr·ªçng** (n·∫øu c·∫ßn thi·∫øt)

### Ti√™u chu·∫©n ch·∫•t l∆∞·ª£ng:
- **ƒê·ªô d√†i**: Kh√¥ng qu√° d√†i (t·ªëi ∆∞u 30-50 t·ª´)
- **Ng√¥n ng·ªØ**: Ti·∫øng Vi·ªát s√∫c t√≠ch, khoa h·ªçc
- **C·∫•u tr√∫c**: Tr·ª±c ti·∫øp, kh√¥ng gi·∫£i th√≠ch d√†i d√≤ng
- **N·ªôi dung**: Ch·ªâ th√¥ng tin thi·∫øt y·∫øu nh·∫•t

### L∆∞u √Ω ƒë·∫∑c bi·ªát:
- KH√îNG b·ªãa ƒë·∫∑t th√¥ng tin kh√¥ng c√≥ trong context
- KH√îNG gi·∫£i th√≠ch chi ti·∫øt n·∫øu kh√¥ng c·∫ßn thi·∫øt
- ∆Øu ti√™n th√¥ng tin th·ª±c ti·ªÖn, c·ª• th·ªÉ
- S·ª≠ d·ª•ng "theo t√†i li·ªáu" khi c·∫ßn thi·∫øt

## TR·∫¢ L·ªúI (NG·∫ÆN G·ªåN):
"""
        
        response = azure_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "B·∫°n l√† chuy√™n gia y t·∫ø chuy√™n tr·∫£ l·ªùi C·ª∞C NG·∫ÆN G·ªåN v√† CH√çNH X√ÅC. Ch·ªâ n√≥i nh·ªØng g√¨ c·∫ßn thi·∫øt nh·∫•t."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
            max_tokens=300
        )
        
        answer = response.choices[0].message.content.strip()
        return answer
        
    except Exception as e:
        context_preview = selected_contexts[0]['content'][:300] if selected_contexts else ""
        return f"D·ª±a tr√™n th√¥ng tin t√¨m ƒë∆∞·ª£c: {context_preview}..."

def search_initial_context_node(state: GraphState) -> GraphState:
    question = state.get("question")
    try:
        query_embedding = get_context_embedding(question)
        results = context_store.similarity_search_with_score(query_embedding=query_embedding, k=10)
        
        if not results:
            return {**state, "error": "Kh√¥ng t√¨m th·∫•y context ph√π h·ª£p."}
        
        initial_chunks = []
        for doc, score in results:
            chunk = {
                "content": doc.page_content,
                "metadata": doc.metadata,
                "score": score,
                "chunk_id": doc.metadata.get("chunk_id", str(uuid.uuid4()))
            }
            initial_chunks.append(chunk)
        
        return {**state, "initial_chunks": initial_chunks}
    except Exception as e:
        return {**state, "error": f"L·ªói khi t√¨m ki·∫øm context: {str(e)}", "initial_chunks": []}

def rerank_context_node(state: GraphState) -> GraphState:
    if state.get("error") or not state.get("initial_chunks"):
        return state
    
    question = state.get("question")
    initial_chunks = state.get("initial_chunks", [])
    reranked_chunks = rerank_documents_api(question, initial_chunks, top_k=5)
    return {**state, "reranked_chunks": reranked_chunks}

def select_context_node(state: GraphState) -> GraphState:
    if state.get("error") or not state.get("reranked_chunks"):
        return state
    
    question = state.get("question")
    reranked_chunks = state.get("reranked_chunks", [])
    selected_contexts = llm_select_context(question, reranked_chunks)
    return {**state, "selected_contexts": selected_contexts}

def generate_answer_node(state: GraphState) -> GraphState:
    if state.get("error"):
        return state
    
    question = state.get("question")
    selected_contexts = state.get("selected_contexts", [])
    answer = llm_generate_answer(question, selected_contexts)
    return {**state, "answer": answer}

def search_images_node(state: GraphState) -> GraphState:
    if state.get("error") or not state.get("selected_contexts"):
        return state
    
    try:
        main_context = state["selected_contexts"][0]['content']
        context_embedding = get_image_embedding(main_context)
        results = image_store.similarity_search_with_score(query_embedding=context_embedding, k=5)
        
        available_images = []
        if results:
            for i, (doc, score) in enumerate(results):
                available_images.append({
                    "id": i,
                    "caption": doc.page_content,
                    "metadata": doc.metadata,
                    "score": score,
                    "image_name": doc.metadata.get("image_name"),
                    "image_path": doc.metadata.get("image_path"), 
                    "image_url": doc.metadata.get("image_url"),
                    "source": doc.metadata.get("source"),
                    "image_id": doc.metadata.get("image_id"),
                    "type": "existing"
                })
        
        return {**state, "available_images": available_images}
    except Exception as e:
        print(f"Image search failed, continuing without images: {str(e)}")
        return {**state, "available_images": []}

def generate_image_node(state: GraphState) -> GraphState:
    if state.get("error"):
        return state
    
    question = state.get("question")
    selected_contexts = state.get("selected_contexts", [])
    answer = state.get("answer")

    if not answer:
        return {**state, "error": "Kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi ƒë·ªÉ t·∫°o ·∫£nh."}

    main_context = selected_contexts[0]['content'] if selected_contexts else ""
    
    generated_image = generate_medical_image(question, main_context, answer)
    return {**state, "generated_image": generated_image}

def finalize_answer_node(state: GraphState) -> GraphState:
    answer = state.get("answer")
    selected_image = state.get("selected_image")
    generated_image = state.get("generated_image")
    user_choice = state.get("user_choice")
    
    if state.get("error"):
        final_answer = f"ƒê√£ c√≥ l·ªói x·∫£y ra: {state.get('error')}"
    else:
        final_answer = answer
        
        if user_choice == "select_existing" and selected_image:
            final_answer += f"\n\nüñºÔ∏è ·∫¢nh minh h·ªça: {selected_image.get('caption', '·∫¢nh li√™n quan')}"
            if selected_image.get('source'):
                final_answer += f"\nNgu·ªìn: {selected_image['source']}"
                
        elif user_choice == "generate_new" and generated_image:
            final_answer += f"\n\nüñºÔ∏è ·∫¢nh ƒë∆∞·ª£c sinh t·ª± ƒë·ªông: {generated_image.get('caption', '·∫¢nh li√™n quan')}"
            final_answer += f"\nNgu·ªìn: {generated_image['source']}"
    
    return {**state, "final_answer": final_answer}

def create_app_rag_graph() -> "CompiledGraph":
    """Create RAG graph optimized for app usage - stops after generating answer and finding images"""
    workflow = StateGraph(GraphState)

    workflow.add_node("search_initial_context", search_initial_context_node)
    workflow.add_node("rerank_context", rerank_context_node)
    workflow.add_node("select_context", select_context_node)
    workflow.add_node("search_images", search_images_node)
    workflow.add_node("generate_answer", generate_answer_node)

    workflow.set_entry_point("search_initial_context")
    workflow.add_edge("search_initial_context", "rerank_context")
    workflow.add_edge("rerank_context", "select_context")
    workflow.add_edge("select_context", "search_images")
    workflow.add_edge("search_images", "generate_answer")

    return workflow.compile()

def create_enhanced_rag_graph() -> "CompiledGraph":
    workflow = StateGraph(GraphState)

    workflow.add_node("search_initial_context", search_initial_context_node)
    workflow.add_node("rerank_context", rerank_context_node)
    workflow.add_node("select_context", select_context_node)
    workflow.add_node("generate_answer", generate_answer_node)
    workflow.add_node("search_images", search_images_node)
    workflow.add_node("generate_image", generate_image_node)
    workflow.add_node("finalize_answer", finalize_answer_node)

    workflow.set_entry_point("search_initial_context")
    workflow.add_edge("search_initial_context", "rerank_context")
    workflow.add_edge("rerank_context", "select_context")
    workflow.add_edge("select_context", "search_images")
    workflow.add_edge("search_images", "generate_answer")
    workflow.add_edge("generate_answer", "finalize_answer")

    return workflow.compile()

def run_image_generation(state: GraphState) -> GraphState:
    """Run only image generation node"""
    return generate_image_node(state)

if __name__ == "__main__":
    graph = create_enhanced_rag_graph()
    test_question = "Paracetamol c√≥ t√°c d·ª•ng g√¨?"
    result = graph.invoke({"question": test_question})
    print("Available images:", len(result.get("available_images", [])))
    print("Final answer:", result["final_answer"])