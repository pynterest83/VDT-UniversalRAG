import gradio as gr
from pipeline import create_enhanced_rag_graph
import os
from PIL import Image
import base64
from io import BytesIO

# Initialize pipeline globally
app = create_enhanced_rag_graph()

def process_medical_question(message, progress=gr.Progress()):
    """Process medical question with detailed step-by-step output"""
    if not message.strip():
        return "Vui l√≤ng nh·∫≠p c√¢u h·ªèi.", "", None, ""
    
    try:
        inputs = {"question": message}
        
        # Initialize outputs
        step_details = []
        final_answer = ""
        image_output = None
        context_info = ""
        
        # Track progress through pipeline
        progress(0, desc="Kh·ªüi t·∫°o pipeline...")
        
        step_count = 0
        total_steps = 6
        
        for output in app.stream(inputs, stream_mode="values"):
            step_count += 1
            progress(step_count / total_steps, desc=f"B∆∞·ªõc {step_count}/{total_steps}")
            
            # Step 1: Initial Context Search
            if output.get("initial_chunks") and not any("B∆Ø·ªöC 1" in detail for detail in step_details):
                initial_chunks = output["initial_chunks"]
                step_details.append(f"‚úÖ **B∆Ø·ªöC 1: T√åM KI·∫æM CONTEXT BAN ƒê·∫¶U**")
                step_details.append(f"   üìÑ T√¨m th·∫•y {len(initial_chunks)} ƒëo·∫°n context t·ª´ vector store")
                step_details.append(f"   üîç Model embedding: bge-m3-v3")
                step_details.append("")
            
            # Step 2: Reranking
            if output.get("reranked_chunks") and not any("B∆Ø·ªöC 2" in detail for detail in step_details):
                reranked_chunks = output["reranked_chunks"]
                step_details.append(f"‚úÖ **B∆Ø·ªöC 2: RERANK CONTEXT V·ªöI VIETNAMESE RERANKER**")
                step_details.append(f"   üîÑ ƒê√£ rerank v√† ch·ªçn top {len(reranked_chunks)} context")
                step_details.append(f"   ü§ñ Model: AITeamVN/Vietnamese_Reranker")
                
                # Show top rerank scores
                for i, chunk in enumerate(reranked_chunks[:3]):
                    score = chunk.get('rerank_score', 0)
                    preview = chunk['content'][:80] + "..." if len(chunk['content']) > 80 else chunk['content']
                    step_details.append(f"      {i+1}. Score: {score:.4f} - {preview}")
                step_details.append("")
            
            # Step 3: Context Selection
            if output.get("selected_contexts") and not any("B∆Ø·ªöC 3" in detail for detail in step_details):
                selected_contexts = output["selected_contexts"]
                step_details.append(f"‚úÖ **B∆Ø·ªöC 3: GPT-4O L·ª∞A CH·ªåN CONTEXT T·ªêT NH·∫§T**")
                step_details.append(f"   üéØ ƒê√£ ch·ªçn {len(selected_contexts)} context t·ª´ {len(output.get('reranked_chunks', []))} context")
                step_details.append(f"   üß† AI ph√¢n t√≠ch v√† ch·ªçn context ph√π h·ª£p nh·∫•t")
                
                # Show selected context preview
                for i, ctx in enumerate(selected_contexts):
                    preview = ctx['content'][:100] + "..." if len(ctx['content']) > 100 else ctx['content']
                    rerank_score = ctx.get('rerank_score', 0)
                    step_details.append(f"      Context {i+1} (Score: {rerank_score:.4f}): {preview}")
                step_details.append("")
            
            # Step 4: Answer Generation
            if output.get("answer") and not any("B∆Ø·ªöC 4" in detail for detail in step_details):
                answer = output["answer"]
                word_count = len(answer.split())
                step_details.append(f"‚úÖ **B∆Ø·ªöC 4: GPT-4O SINH C√ÇU TR·∫¢ L·ªúI**")
                step_details.append(f"   üí¨ ƒê√£ sinh c√¢u tr·∫£ l·ªùi ({word_count} t·ª´)")
                step_details.append(f"   üéØ D·ª±a tr√™n {len(output.get('selected_contexts', []))} context ƒë∆∞·ª£c ch·ªçn")
                step_details.append("")
            
            # Step 5: Image Search
            if output.get("image_info") and not any("B∆Ø·ªöC 5" in detail for detail in step_details):
                image_info = output["image_info"]
                step_details.append(f"‚úÖ **B∆Ø·ªöC 5: T√åM KI·∫æM ·∫¢NH MINH H·ªåA**")
                step_details.append(f"   üñºÔ∏è T√¨m th·∫•y ·∫£nh li√™n quan v·ªõi score: {image_info.get('score', 0):.4f}")
                step_details.append(f"   üì∏ Model embedding: bge-m3-image")
                step_details.append(f"   üìù Caption: {image_info.get('caption', 'N/A')[:100]}...")
                if image_info.get('image_name'):
                    step_details.append(f"   üìÅ T√™n ·∫£nh: {image_info['image_name']}")
                step_details.append("")
            
            # Step 6: Final Answer
            if output.get("final_answer") and not any("B∆Ø·ªöC 6" in detail for detail in step_details):
                step_details.append(f"‚úÖ **B∆Ø·ªöC 6: HO√ÄN THI·ªÜN C√ÇU TR·∫¢ L·ªúI**")
                step_details.append(f"   ‚ú® K·∫øt h·ª£p c√¢u tr·∫£ l·ªùi + th√¥ng tin ·∫£nh")
                step_details.append(f"   üèÅ Pipeline ho√†n t·∫•t!")
                step_details.append("")
        
        # Get final outputs
        final_output = output
        
        if final_output and final_output.get("final_answer"):
            final_answer = final_output["final_answer"]
            
            # Process image if available
            image_info = final_output.get("image_info")
            if image_info:
                image_output = process_image_output(image_info)
            
            # Create context information
            context_info = create_context_summary(final_output)
            
        else:
            final_answer = "Xin l·ªói, t√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y."
        
        # Combine step details
        step_display = "\n".join(step_details)
        
        return final_answer, step_display, image_output, context_info
            
    except Exception as e:
        error_msg = f"ƒê√£ x·∫£y ra l·ªói: {str(e)}"
        return error_msg, f"‚ùå L·ªói trong qu√° tr√¨nh x·ª≠ l√Ω: {str(e)}", None, ""

def process_image_output(image_info):
    """Process image information to display in Gradio"""
    if not image_info:
        return None
    
    try:
        # Try to load image from various sources
        image_path = image_info.get('image_path')
        image_url = image_info.get('image_url')
        image_name = image_info.get('image_name')
        
        # Try local path first
        if image_path and os.path.exists(image_path):
            return Image.open(image_path)
        
        # Try alternative paths
        possible_paths = [
            f"images/{image_name}" if image_name else None,
            f"datasets/images/{image_name}" if image_name else None,
            f"./images/{image_name}" if image_name else None,
        ]
        
        for path in possible_paths:
            if path and os.path.exists(path):
                return Image.open(path)
        
        # If no image found, return placeholder info
        return None
        
    except Exception as e:
        print(f"Error loading image: {e}")
        return None

def create_context_summary(output):
    """Create summary of context and processing information"""
    try:
        summary_parts = []
        
        # Question info
        question = output.get("question", "N/A")
        summary_parts.append(f"**C√ÇU H·ªéI:** {question}")
        summary_parts.append("")
        
        # Context statistics
        initial_chunks = output.get("initial_chunks", [])
        reranked_chunks = output.get("reranked_chunks", [])
        selected_contexts = output.get("selected_contexts", [])
        
        summary_parts.append("**TH·ªêNG K√ä CONTEXT:**")
        summary_parts.append(f"‚Ä¢ Context ban ƒë·∫ßu: {len(initial_chunks)} ƒëo·∫°n")
        summary_parts.append(f"‚Ä¢ Context sau rerank: {len(reranked_chunks)} ƒëo·∫°n")
        summary_parts.append(f"‚Ä¢ Context ƒë∆∞·ª£c ch·ªçn: {len(selected_contexts)} ƒëo·∫°n")
        summary_parts.append("")
        
        # Selected contexts details
        if selected_contexts:
            summary_parts.append("**CONTEXT ƒê∆Ø·ª¢C S·ª¨ D·ª§NG:**")
            for i, ctx in enumerate(selected_contexts):
                score = ctx.get('rerank_score', 0)
                content_preview = ctx['content'][:200] + "..." if len(ctx['content']) > 200 else ctx['content']
                summary_parts.append(f"**Context {i+1}** (Rerank Score: {score:.4f})")
                summary_parts.append(f"{content_preview}")
                summary_parts.append("")
        
        # Image info
        image_info = output.get("image_info")
        if image_info:
            summary_parts.append("**TH√îNG TIN ·∫¢NH:**")
            summary_parts.append(f"‚Ä¢ Caption: {image_info.get('caption', 'N/A')}")
            summary_parts.append(f"‚Ä¢ Score: {image_info.get('score', 0):.4f}")
            if image_info.get('source'):
                summary_parts.append(f"‚Ä¢ Ngu·ªìn: {image_info['source']}")
            summary_parts.append("")
        
        return "\n".join(summary_parts)
        
    except Exception as e:
        return f"L·ªói t·∫°o t√≥m t·∫Øt: {str(e)}"

def create_interface():
    """Create the main Gradio interface"""
    
    with gr.Blocks(title="üè• Medical RAG Chatbot", theme=gr.themes.Soft()) as demo:
        
        gr.Markdown("""
        # üè• Medical RAG Chatbot v·ªõi GPT-4o
        
        **H·ªá th·ªëng h·ªèi ƒë√°p y t·∫ø th√¥ng minh** s·ª≠ d·ª•ng:
        - üîç **BGE-M3** cho t√¨m ki·∫øm context 
        - üáªüá≥ **Vietnamese Reranker** cho s·∫Øp x·∫øp l·∫°i
        - üß† **GPT-4o** cho l·ª±a ch·ªçn context v√† tr·∫£ l·ªùi
        - üñºÔ∏è **BGE-M3-Image** cho t√¨m ki·∫øm ·∫£nh minh h·ªça
        
        ƒê·∫∑t c√¢u h·ªèi v·ªÅ y t·∫ø, thu·ªëc, b·ªánh, th·∫£o d∆∞·ª£c, v√† nh·∫≠n c√¢u tr·∫£ l·ªùi chi ti·∫øt k√®m h√¨nh ·∫£nh!
        """)
        
        with gr.Row():
            with gr.Column(scale=2):
                # Input section
                question_input = gr.Textbox(
                    label="üí¨ C√¢u h·ªèi y t·∫ø",
                    placeholder="VD: Thu·ªëc Paracetamol c√≥ t√°c d·ª•ng ph·ª• g√¨?",
                    lines=2
                )
                
                with gr.Row():
                    submit_btn = gr.Button("üöÄ H·ªèi", variant="primary", scale=2)
                    clear_btn = gr.Button("üóëÔ∏è X√≥a", scale=1)
                
                # Examples
                gr.Examples(
                    examples=[
                        "B√°ch th·∫£o s∆∞∆°ng d√πng ƒë·ªÉ ch·ªØa ch·∫£y m√°u k·∫Ω rƒÉng nh∆∞ th·∫ø n√†o?",
                        "Thu·ªëc Paracetamol c√≥ t√°c d·ª•ng g√¨ v√† li·ªÅu d√πng ra sao?",
                        "Calci D Hasan c√≥ th·ªÉ g√¢y ·∫£nh h∆∞·ªüng g√¨ ƒë·∫øn b·ªánh nh√¢n m·∫Øc b·ªánh l√Ω n·ªÅn?",
                        "C√¢y l√° l·ªët c√≥ c√¥ng d·ª•ng g√¨ trong y h·ªçc c·ªï truy·ªÅn?",
                        "Vitamin D thi·∫øu h·ª•t c√≥ tri·ªáu ch·ª©ng g√¨?",
                    ],
                    inputs=question_input
                )
            
            with gr.Column(scale=1):
                # Image output
                image_output = gr.Image(
                    label="üñºÔ∏è ·∫¢nh minh h·ªça",
                    height=300
                )
        
        with gr.Row():
            with gr.Column():
                # Main answer
                answer_output = gr.Textbox(
                    label="üí° C√¢u tr·∫£ l·ªùi",
                    lines=8,
                    max_lines=15
                )
            
            with gr.Column():
                # Step-by-step process
                steps_output = gr.Textbox(
                    label="‚öôÔ∏è C√°c b∆∞·ªõc th·ª±c hi·ªán",
                    lines=8,
                    max_lines=15
                )
        
        # Context details (collapsible)
        with gr.Accordion("üìä Chi ti·∫øt Context & Th√¥ng tin", open=False):
            context_output = gr.Textbox(
                label="Th√¥ng tin chi ti·∫øt",
                lines=10,
                max_lines=20
            )
        
        # Event handlers
        def submit_question(question):
            if not question.strip():
                return "Vui l√≤ng nh·∫≠p c√¢u h·ªèi.", "", None, ""
            return process_medical_question(question)
        
        def clear_all():
            return "", "", None, "", ""
        
        submit_btn.click(
            fn=submit_question,
            inputs=[question_input],
            outputs=[answer_output, steps_output, image_output, context_output]
        )
        
        question_input.submit(
            fn=submit_question,
            inputs=[question_input],
            outputs=[answer_output, steps_output, image_output, context_output]
        )
        
        clear_btn.click(
            fn=clear_all,
            outputs=[question_input, answer_output, steps_output, image_output, context_output]
        )
        
        # Footer
        gr.Markdown("""
        ---
        **üîß Technical Stack:** BGE-M3 ‚Ä¢ Vietnamese Reranker ‚Ä¢ GPT-4o ‚Ä¢ Qdrant ‚Ä¢ LangGraph
        
        **‚ö†Ô∏è L∆∞u √Ω:** Th√¥ng tin ch·ªâ mang t√≠nh tham kh·∫£o. Vui l√≤ng tham kh·∫£o √Ω ki·∫øn b√°c sƒ© cho ch·∫©n ƒëo√°n v√† ƒëi·ªÅu tr·ªã ch√≠nh x√°c.
        """)
    
    return demo

if __name__ == "__main__":
    demo = create_interface()
    demo.launch(
        share=True,
        server_name="0.0.0.0",
        server_port=7860
    )
